# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Code Sandbox (SandboxFusion) - A secure sandbox for running and judging code generated by LLMs. Supports 30+ languages including Python, C++, Go, Java, Node.js, CUDA, and more.

## Development Commands

```bash
# Install dependencies
conda create -n sandbox -y python=3.12
conda activate sandbox
poetry install

# Run the server (development with auto-reload)
make run

# Run the server in production mode (no auto-reload)
make run-online

# Run all tests (excludes cuda, datalake, dp_eval, lean)
make test

# Run tests in parallel with 4 workers
make test TEST_NP=4

# Run a specific test by name
make test-case CASE=test_java_assert

# Run a specific test with pdb debugging
make test-case-pdb CASE=test_java_assert

# Run GPU tests
make test-cuda

# Run minor language tests (lua, R, perl, ruby, etc.)
make test-minor

# Run verilog tests
make test-verilog

# Run verilog tests with pdb debugging
make test-verilog-pdb

# Format code
make format

# Check formatting and run tests
make check
```

**Test Markers** (configured in `pyproject.toml`):
- `cuda`: Tests requiring GPU
- `minor`: Tests for minor/scripting languages
- `verilog`: Verilog HDL tests
- `datalake`: Tests requiring online datalake access
- `dp_eval`: Tests using external dp ada API
- `lean`: Lean prover tests

## Architecture

### Server Layer (`sandbox/server/`)
- **server.py**: FastAPI application entry point with lifespan management for database connections. Mounts `/SandboxFusion` (docs) and `/playground` (UI)
- **sandbox_api.py**: `/run_code` and `/run_jupyter` endpoints for executing code snippets
- **online_judge_api.py**: API for running code evaluation benchmarks

### Jupyter Mode
The `/run_jupyter` endpoint runs code in a persistent Jupyter kernel (kernel: `python3`):
- Executes cells sequentially in order
- Each cell can produce stdout, stderr, display output, and errors
- Kernel state persists across cells within a single request

### Runner Architecture (`sandbox/runners/`)
Code execution is dispatched through a registry pattern:

```python
CODE_RUNNERS = { **MAJOR_RUNNERS, **MINOR_RUNNERS, **GPU_RUNNERS }
```

**Major Runners** (`major.py`): Complex languages with compilation/dependency setup
- `run_cpp`, `run_go`, `run_go_test`, `run_java`, `run_junit`, `run_nodejs`, `run_typescript`, `run_jest`, `run_python`, `run_pytest`, `run_rust`, `run_php`, `run_bash`, `run_csharp`

**Minor Runners** (`minor.py`): Simpler scripting languages without build steps
- `run_lua`, `run_r`, `run_perl`, `run_ruby`, `run_scala`, `run_julia`, `run_d`, `run_swift`, `run_racket`, `run_lean`, `run_verilog`, `run_kotlin_script`

**GPU Runners** (`cuda.py`): CUDA and Python GPU execution
- `run_cuda`, `run_python_gpu`

**Jupyter Runner** (`jupyter.py`): Cell-by-cell notebook execution via Jupyter kernel

**Runner Interface**: Each runner is `async def run_<lang>(args: CodeRunArgs) -> CodeRunResult`

### Base Execution (`sandbox/runners/base.py`)
- `run_command_bare()`: Low-level command execution using asyncio subprocess
- `run_commands()`: High-level execution with compile/run stages
- `restore_files()`: Writes base64-encoded files to disk

### Isolation System (`sandbox/runners/isolation.py`)
Three isolation strategies configured via `sandbox_config.isolation`:
- **'none'**: Basic execution with resource limits (RLIMIT_AS/DATA) via Python's `resource` module
- **'lite'**: Combines overlayfs (filesystem), cgroups (resource limits), chroot (jail), and network namespaces

### Configuration (`sandbox/configs/`)
- **run_config.py**: Pydantic-based configuration model
- `local.yaml`: Development configuration
- `ci.yaml`: CI environment configuration
- Selected via `SANDBOX_CONFIG` env var (defaults to 'local')

### Utilities (`sandbox/utils/`)
- **execution.py**: Process cleanup, bash integrity verification, concurrency limiting
- **antihack.py**: Security measures to prevent sandbox escapes (see `sandbox/utils/antihack.py`)
- **extraction.py**: Code parsing utilities (e.g., finding Java public class names)
- **logging.py**: Structlog-based structured logging configuration

### Datasets & Evaluation (`sandbox/datasets/`)
The `CodingDataset` abstract base class defines the evaluation protocol:
- `get_prompts()`: Retrieve problem prompts from dataset
- `get_prompt_by_id()`: Get single prompt by ID
- `evaluate_single()`: Submit code completion for evaluation

**Registered Datasets** (see `sandbox/configs/ci.yaml`):
- HumanEval (multi-language: Python, C++, TypeScript, Java, Go, C#, Bash)
- MBPP, MBXP, MHPP, CodeContests, CruxEval
- BigCodeBench, LiveCodeBench, NaturalCodeBench
- Repobench (C and Python)
- VerilogEval, PAL-Math, MiniF2F (Lean4)
- Custom: AutoEvalDataset, CommonOJDataset

**Dataset Registry** (`sandbox/registry.py`):
- Maps dataset IDs to `CodingDataset` implementations via config
- Supports dynamic loading via module path + class name

### Database Layer (`sandbox/database.py`)
Dual-database setup for benchmark evaluation:
- **Datalake** (MySQL): Persistent storage for dataset content (configured in YAML)
- **SQLite**: Ephemeral in-memory storage for evaluation results

## Key Patterns

1. **Runner Implementation**: Async function `run_<lang>(args: CodeRunArgs) -> CodeRunResult`
2. **Temporary Files**: Use `tempfile.TemporaryDirectory(dir=get_tmp_dir())` for execution workspaces
3. **Async All the Way**: All I/O operations use async versions (aiofiles, asyncio subprocess)
4. **Singletons**: `RunConfig.get_instance_sync()` for configuration access
5. **Registry Pattern**: `CODE_RUNNERS` dict maps language names to runner functions; datasets registered via config YAML